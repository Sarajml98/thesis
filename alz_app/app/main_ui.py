"""Streamlit app for AlzMultiModal Analyzer.

Run: streamlit run app/main_ui.py

The app uses `backend.run_all_analyses` and displays per-module cards and
an aggregated textual conclusion generated by `summaries.build_final_summary`.
"""
import streamlit as st
from pathlib import Path
import sys, os
# Ensure parent folder is on sys.path so 'app' package imports work when running this file directly
BASE_DIR = Path(__file__).resolve().parents[1]
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))
from app import backend, summaries

st.set_page_config(page_title="AlzMultiModal Analyzer", layout="wide", initial_sidebar_state="expanded")

# ----- Sidebar -----
st.sidebar.title("AlzMultiModal Analyzer")
st.sidebar.caption("Multi-modal AD analysis (demo). Choose data folder and run analyses.")

default_data = str(Path.cwd() / "data")
if "data_root" not in st.session_state:
    st.session_state["data_root"] = default_data

data_root = st.sidebar.text_input("Data root folder", value=st.session_state["data_root"])
if st.sidebar.button("Select data folder"):
    # For now, we accept manual path input; users can paste or type the folder path.
    st.session_state["data_root"] = data_root

st.sidebar.markdown("---")
st.sidebar.subheader("Modalities")
st.sidebar.write("- MRI+PET (TransMF_AD)")
st.sidebar.write("- EEG (LEAD)")
st.sidebar.write("- ADNI (R scripts + CNN)")
st.sidebar.write("- TADPOLE helper scripts")
st.sidebar.write("- Proteomics (AD-Biomarkers-Project)")

st.sidebar.markdown("---")
simulate = st.sidebar.checkbox("Simulate missing external repos (safe demo)", value=True)

# ----- Main area -----
st.title("AlzMultiModal Analyzer")
st.markdown("A simple, modern UI to run multiple AD research pipelines and summarize results.")

run_col, info_col = st.columns([3, 1])
with run_col:
    st.markdown("---")
    run_all = st.button("▶️  Run All Analyses", key="run_all", help="Run all 5 pipelines in sequence")
    progress_placeholder = st.empty()
with info_col:
    st.info("Use the sidebar to set the data root and options.")

# placeholders for module cards
cards = {
    "mri_pet": st.empty(),
    "eeg": st.empty(),
    "adni": st.empty(),
    "tadpole": st.empty(),
    "proteomics": st.empty(),
}

final_placeholder = st.empty()

# simple UI progress hook used by backend
progress_bar = st.progress(0)


def ui_progress(module_name, status, fraction, details):
    # Update small card for the module
    content = f"**{module_name.upper()}** — {status}\n\n{details}"
    if status in ("starting", "queued", "running", "simulating"):
        cards[module_name].info(content)
    elif status in ("completed", "success"):
        cards[module_name].success(content)
    elif status == "error":
        cards[module_name].error(content)
    else:
        cards[module_name].write(content)
    progress_bar.progress(min(max(int(fraction * 100), 0), 100))


if run_all:
    if not data_root:
        st.warning("Please set a data root folder in the sidebar before running.")
    else:
        with st.spinner("Running pipelines... this may take a while depending on external code"):
            results = backend.run_all_analyses(data_root, simulate_if_missing=simulate, progress_hook=ui_progress)
        st.success("All done!")

        # Show summarized metrics per module (if available)
        cols = st.columns(5)
        order = ["mri_pet", "eeg", "adni", "tadpole", "proteomics"]
        for c, name in zip(cols, order):
            res = results.get(name, {"status": "error"})
            with c:
                st.subheader(name.upper())
                if res.get("status") == "success":
                    # choose two numbers to show as metrics if found
                    metric_1 = res.get("accuracy") or (res.get("metrics") or {}).get("accuracy")
                    metric_2 = res.get("auc") or (res.get("metrics") or {}).get("auc")
                    if metric_1 is not None:
                        st.metric("Accuracy", f"{metric_1:.2f}")
                    if metric_2 is not None:
                        st.metric("AUC", f"{metric_2:.2f}")
                    st.caption(res.get("interpretation", ""))
                else:
                    st.write("No result — check logs or ensure external repo is present.")

        # Final combined summary
        final_text = summaries.build_final_summary(results)
        final_placeholder.markdown("---")
        st.header("Final Conclusion")
        st.write(final_text)

        # Save final summary
        out_file = Path.cwd() / "outputs" / "final_summary.txt"
        out_file.parent.mkdir(parents=True, exist_ok=True)
        out_file.write_text(final_text)
        st.success(f"Final summary saved to {out_file}")

        # Keep results in session state for later subject checks
        st.session_state["last_results"] = results

        # Results saved in session to use later (see "Check Individual Subject" below)
        st.session_state["last_results"] = results
        st.success("Results saved — use 'Check Individual Subject' panel below to inspect individual reports.")

# If not running, show current placeholders with 'Pending' state
else:
    for name, p in cards.items():
        p.info(f"**{name.upper()}** — Pending\n\nClick 'Run All Analyses' to start.")
    progress_bar.progress(0)

# ⚡ Bolt: Cache disk I/O to improve app responsiveness.
# This function reads multiple files from disk, which is a slow operation.
# Caching the result prevents the app from re-reading the same data on every
# user interaction, making the UI much faster.
@st.cache_data
def load_previous_results():
    """Loads previous analysis results from the outputs directory."""
    results = {}
    import json
    import csv
    out_dir = Path.cwd() / "outputs"
    agg = out_dir / "aggregate_results.json"
    if agg.exists():
        try:
            with open(agg, "r", encoding="utf-8") as f:
                results = json.load(f)
            st.info("Loaded previous results from outputs/aggregate_results.json")
        except Exception:
            results = {}
    else:
        # try to load per-module summary files
        for name in ["mri_pet", "eeg", "adni", "tadpole", "proteomics"]:
            summary_path = out_dir / f"{name}_summary.json"
            if summary_path.exists():
                try:
                    with open(summary_path, "r", encoding="utf-8") as f:
                        results[name] = json.load(f)
                    # try to attach predictions if present
                    preds_path = out_dir / f"{name}_predictions.csv"
                    if preds_path.exists():
                        rows = []
                        with open(preds_path, "r", encoding="utf-8") as pf:
                            reader = csv.DictReader(pf)
                            for row in reader:
                                # normalize probability
                                row["probability"] = float(row.get("probability", 0.0))
                                rows.append({"subject_id": row.get("subject_id"), "predicted_label": row.get("predicted_label"), "probability": row.get("probability")})
                        results[name]["predictions"] = rows
                except Exception:
                    pass
    return results


# --- Persistent Subject-level check ---
st.markdown("---")
st.subheader("Check Individual Subject")
results = st.session_state.get("last_results")
# If no results in session, try to load from outputs/aggregate_results.json or individual summaries
if not results:
    results = load_previous_results()
    if results:
        st.session_state["last_results"] = results
    else:
        st.info("No results available. Run 'Run All Analyses' to produce results, or load previous results.")

# build subject list from available predictions across modules
subjects_set = set()
for r in results.values():
    for p in (r.get("predictions") or []):
        subjects_set.add(p.get("subject_id"))
if not subjects_set:
    subjects_list = [f"SUBJ{str(i).zfill(3)}" for i in range(1, 11)]
else:
    subjects_list = sorted(subjects_set)
selected_subject = st.selectbox("Select subject ID", options=subjects_list, key="selected_subject")
if st.button("Check Subject", key="check_subject"):
    report = backend.predict_subject(selected_subject, results)
    st.session_state["last_report"] = report

if "last_report" in st.session_state:
    report = st.session_state["last_report"]
    ensemble = report.get("ensemble", {})
    if ensemble.get("probability") is not None:
        st.success(f"Ensemble prediction: {ensemble['label']} (prob = {ensemble['probability']:.3f})")
        if report.get("final_text"):
            st.markdown(f"**{report.get('final_text')}**")
        if report.get("disclaimer"):
            st.caption(report.get("disclaimer"))
    else:
        st.warning("Ensemble prediction: unknown (no per-module predictions available)")

    st.markdown("**Per-module predictions**")
    per = report.get("per_module", {})
    cols = st.columns(max(1, len(per)))
    for (m, v), col in zip(per.items(), cols):
        with col:
            if v.get("status") == "ok":
                st.metric(m.upper(), f"{v['label']}", f"P={v['probability']:.3f}")
            else:
                st.write(f"{m.upper()}: No prediction")
    st.write("Detailed report saved to:", report.get("report_path"))
    st.json(report)

    if st.button("Clear saved results", key="clear_results"):
        st.session_state.pop("last_results", None)
        st.session_state.pop("last_report", None)
        st.success("Saved results cleared.")

# footer
st.markdown("---")
st.caption("Designed for demo purposes — replace simulation flags with real pipelines after cloning external repos.")
